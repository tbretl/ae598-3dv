{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For input/output\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# For numerical methods\n",
    "import numpy as np\n",
    "\n",
    "# For image processing and visualization of results\n",
    "import cv2\n",
    "from pupil_apriltags import Detector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For optimization with symforce\n",
    "import symforce\n",
    "symforce.set_epsilon_to_symbol()\n",
    "import symforce.symbolic as sf\n",
    "from symforce.values import Values\n",
    "from symforce.opt.factor import Factor\n",
    "from symforce.opt.optimizer import Optimizer\n",
    "import sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say where things are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory\n",
    "data_dir = Path('data')\n",
    "\n",
    "# Where images are read from\n",
    "img_src_dir = Path(data_dir, 'calibration_images')\n",
    "\n",
    "# Where images are written to\n",
    "img_dst_dir = Path(data_dir, 'calibration_results')\n",
    "\n",
    "# Where the calibration template is located\n",
    "template_filename = Path(data_dir, 'tag36_11_grid_5x8-template.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get a tag with a particular ID from the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_with_id(tag_id, template):\n",
    "    for tag in template['tags']:\n",
    "        if tag['tag_id'] == tag_id:\n",
    "            return tag\n",
    "    raise Exception(f'tag_id {tag_id} not found in template')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(template_filename, 'r') as f:\n",
    "    template = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tag detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_detector = Detector(\n",
    "    families=template['tag_family'],\n",
    "    nthreads=1,\n",
    "    quad_decimate=1.0,\n",
    "    quad_sigma=0.0,\n",
    "    refine_edges=1,\n",
    "    decode_sharpening=0.,\n",
    "    debug=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect tags in all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag corners must be no less than this number of pixels from the image border\n",
    "buffer_px = 10\n",
    "\n",
    "# We are going to create a list of views, one per image\n",
    "views = []\n",
    "\n",
    "# Iterate over all images in the source directory\n",
    "for image_path in img_src_dir.iterdir():\n",
    "    # Skip anything that isn't a PNG file\n",
    "    if (not image_path.is_file()) or (image_path.suffix.lower() != '.png'):\n",
    "        continue\n",
    "\n",
    "    # Read image as grayscale\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Detect tags\n",
    "    tag_detections = tag_detector.detect(\n",
    "        img,\n",
    "        estimate_tag_pose=False,\n",
    "        camera_params=None,\n",
    "        tag_size=None,\n",
    "    )\n",
    "\n",
    "    # Get n point correspondences:\n",
    "    #\n",
    "    #  p (n x 3) is coordinates of each point in the tag frame\n",
    "    #  q (n x 2) is coordinates of each point in the image frame\n",
    "    #\n",
    "    rejected_tags = []\n",
    "    tags = []\n",
    "    p = []\n",
    "    q = []\n",
    "    for d in tag_detections:\n",
    "        # Reject tags with corners too close to the image boundary\n",
    "        if ((d.corners[:, 0] < buffer_px).any() or\n",
    "            (d.corners[:, 0] > (img.shape[1] - 1) - buffer_px).any() or\n",
    "            (d.corners[:, 1] < buffer_px).any() or\n",
    "            (d.corners[:, 1] > (img.shape[0] - 1) - buffer_px).any()):\n",
    "            continue\n",
    "        \n",
    "        # Add tag to list of detected tags\n",
    "        tags.append({\n",
    "            'tag_id': d.tag_id,\n",
    "            'corners': d.corners.tolist(),\n",
    "        })\n",
    "\n",
    "        # Add corners of tag to point correspondences\n",
    "        p.extend(get_tag_with_id(d.tag_id, template)['corners'])\n",
    "        q.extend(d.corners.tolist())\n",
    "    \n",
    "    # Make sure the lengths of p and q are consistent\n",
    "    assert(len(p) == len(q))\n",
    "    \n",
    "    # Count the number of tags and correspondences that were found\n",
    "    num_tags = len(tags)\n",
    "    num_points = len(p)\n",
    "\n",
    "    # Add to the list of views\n",
    "    views.append({\n",
    "        'image_name': str(image_path.name),\n",
    "        'num_tags': num_tags,\n",
    "        'tags': tags,\n",
    "        'num_points': num_points,\n",
    "        'p': p,\n",
    "        'q': q,\n",
    "    })\n",
    "    \n",
    "    # Show results\n",
    "    print(f' {len(views) - 1:3d} ' +\n",
    "          f': {str(image_path):30s} ' +\n",
    "          f': {num_tags:3d} tags ({len(rejected_tags):3d} rejected) ' +\n",
    "          f': {num_points:3d} points ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate intrinsic and extrinsic parameters by inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the intrinsic parameters (i.e., $K$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (1)\n",
    "K = np.array([\n",
    "    [1., 0., 0.],\n",
    "    [0., 1., 0.],\n",
    "    [0., 0., 1.],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the extrinsic parameters (i.e., the pose $T_{camera}^{world}$ for each view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (2)\n",
    "poses = []\n",
    "for view in views:\n",
    "    poses.append(np.array([\n",
    "        [1., 0., 0., 0.],\n",
    "        [0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0.],\n",
    "        [0., 0., 0., 1.],\n",
    "    ]))\n",
    "poses = np.array(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate intrinsic and extrinsic parameters by analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that implements the wedge operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew(v):\n",
    "    assert(type(v) == np.ndarray)\n",
    "    assert(v.shape == (3,))\n",
    "    return np.array([[0., -v[2], v[1]],\n",
    "                     [v[2], 0., -v[0]],\n",
    "                     [-v[1], v[0], 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to estimate the planar homography (i.e., $H$) between two sets of points. The \"source points\" (`pts_src`) are on the tag grid and are expressed in the coordinates of the world frame. The \"destination points\" (`pts_dst`) are in the image and are expressed in image coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (5)\n",
    "def get_homography(pts_src, pts_dst):\n",
    "    return np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get the homography for each view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homographies(views):\n",
    "    homographies = []\n",
    "    for view in views:\n",
    "        # Get src points (tag)\n",
    "        pts_src = []\n",
    "        for p in view['p']:\n",
    "            pts_src.append(p[:-1])\n",
    "        pts_src = np.array(pts_src)\n",
    "\n",
    "        # Get dst points (img)\n",
    "        pts_dst = []\n",
    "        for q in view['q']:\n",
    "            pts_dst.append(q)\n",
    "        pts_dst = np.array(pts_dst)\n",
    "\n",
    "        # Get homography\n",
    "        homographies.append(get_homography(pts_src, pts_dst))\n",
    "    return np.array(homographies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get the intrinsic parameters (i.e., the intrinsic camera matrix $K$), given homographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (6)\n",
    "def get_intrinsic_parameters(homographies):\n",
    "    return np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get the extrinsic parameters (i.e., the camera pose for each view), given homographics and intrinsic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (7)\n",
    "def get_extrinsic_parameters(homographies, K):\n",
    "    poses = []\n",
    "    for view in views:\n",
    "        poses.append(np.array([\n",
    "            [1., 0., 0., 0.],\n",
    "            [0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0.],\n",
    "            [0., 0., 0., 1.],\n",
    "        ]))\n",
    "    \n",
    "    # Return all poses\n",
    "    return np.array(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply code to perform intrinsic and extrinsic calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homographies = get_homographies(views)\n",
    "K = get_intrinsic_parameters(homographies)\n",
    "poses = get_extrinsic_parameters(homographies, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(linewidth=150, formatter={'float': lambda x: f'{x:10.4f}'}):\n",
    "    print('K')\n",
    "    print(K)\n",
    "    print('')\n",
    "\n",
    "for view, pose in zip(views, poses):\n",
    "    with np.printoptions(linewidth=150, formatter={'float': lambda x: f'{x:10.4f}'}):\n",
    "        print(f'Camera pose for image {view[\"image_name\"]}')\n",
    "        print(pose)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate intrinsic and extrinsic parameters by optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a symbolic function that projects a point into the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (3)\n",
    "def projection(\n",
    "    T: sf.Pose3,\n",
    "    p: sf.V3,\n",
    "    fx: sf.Scalar,\n",
    "    fy: sf.Scalar,\n",
    "    cx: sf.Scalar,\n",
    "    cy: sf.Scalar,\n",
    "    epsilon: sf.Scalar,\n",
    ") -> sf.V2:\n",
    "    return sf.V2(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a symbolic function that computes the difference between a projected point and an image point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (4)\n",
    "def projection_residual(\n",
    "    T: sf.Pose3,\n",
    "    p: sf.V3,\n",
    "    q: sf.V2,\n",
    "    fx: sf.Scalar,\n",
    "    fy: sf.Scalar,\n",
    "    cx: sf.Scalar,\n",
    "    cy: sf.Scalar,\n",
    "    epsilon: sf.Scalar,  \n",
    ") -> sf.V2:\n",
    "    return sf.V2(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambdify these two functions so they can be evaluated numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_num = symforce.util.lambdify(projection)\n",
    "projection_residual_num = symforce.util.lambdify(projection_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial values for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_values = Values(\n",
    "    views=[],               # <-- fill this list with initial values specific to each view\n",
    "    fx=K[0, 0],             # <-- initial guess at fx\n",
    "    fy=K[1, 1],             # <-- initial guess at fy\n",
    "    cx=K[0, 2],             # <-- initial guess at cx\n",
    "    cy=K[1, 2],             # <-- initial guess at cy\n",
    "    epsilon=sym.epsilon,    # <-- constant parameter required by symforce\n",
    ")\n",
    "\n",
    "# Iterate over each view (along with each camera pose estimate)\n",
    "for view, pose in zip(views, poses):\n",
    "    view_values = Values(\n",
    "        T=sym.Pose3(        # <-- initial guess at camera pose\n",
    "            R=sym.Rot3.from_rotation_matrix(pose[0:3, 0:3]),\n",
    "            t=pose[0:3, 3],\n",
    "        ),\n",
    "        matches=[],         # <-- fill this list with initial values specific to each match\n",
    "    )\n",
    "\n",
    "    # Iterate over each match (i.e., each point correspondence)\n",
    "    for p, q in zip(view['p'], view['q']):\n",
    "        view_values['matches'].append(Values(p=np.array(p), q=np.array(q)))\n",
    "    \n",
    "    # Append the initial values we just created to the list of views\n",
    "    initial_values['views'].append(view_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create factors for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "for i_view, view in enumerate(initial_values['views']):\n",
    "    for i_match, match in enumerate(view['matches']):\n",
    "        factors.append(Factor(\n",
    "            residual=projection_residual,\n",
    "            keys=[\n",
    "                f'views[{i_view}].T',\n",
    "                f'views[{i_view}].matches[{i_match}].p',\n",
    "                f'views[{i_view}].matches[{i_match}].q',\n",
    "                'fx',\n",
    "                'fy',\n",
    "                'cx',\n",
    "                'cy',\n",
    "                'epsilon',\n",
    "            ]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_keys = ['fx', 'fy', 'cx', 'cy',]\n",
    "for i_view, view in enumerate(initial_values['views']):\n",
    "    optimized_keys.append(f'views[{i_view}].T')\n",
    "optimizer = Optimizer(\n",
    "    factors=factors,\n",
    "    optimized_keys=optimized_keys,\n",
    "    debug_stats=True,\n",
    "    params=Optimizer.Params(\n",
    "        iterations=100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimizer.optimize(initial_values)\n",
    "assert(result.status == Optimizer.Status.SUCCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show sum-squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all errors before optimization\n",
    "initial_errors = []\n",
    "for view in result.initial_values['views']:\n",
    "    for match in view['matches']:\n",
    "        initial_errors.append(np.linalg.norm(projection_residual_num(\n",
    "            view['T'],\n",
    "            match['p'],\n",
    "            match['q'],\n",
    "            result.initial_values['fx'],\n",
    "            result.initial_values['fy'],\n",
    "            result.initial_values['cx'],\n",
    "            result.initial_values['cy'],\n",
    "            result.initial_values['epsilon'],\n",
    "        )))\n",
    "initial_errors = np.array(initial_errors)\n",
    "\n",
    "# Compute all errors after optimization\n",
    "final_errors = []\n",
    "for view in result.optimized_values['views']:\n",
    "    for match in view['matches']:\n",
    "        final_errors.append(np.linalg.norm(projection_residual_num(\n",
    "            view['T'],\n",
    "            match['p'],\n",
    "            match['q'],\n",
    "            result.optimized_values['fx'],\n",
    "            result.optimized_values['fy'],\n",
    "            result.optimized_values['cx'],\n",
    "            result.optimized_values['cy'],\n",
    "            result.optimized_values['epsilon'],\n",
    "        )))\n",
    "final_errors = np.array(final_errors)\n",
    "\n",
    "# Compute sum-squared errors\n",
    "print(f'Sum-squared error (halved), before optimization: {0.5 * np.sum(initial_errors**2):.1f} pixels')\n",
    "print(f'Sum-squared error (halved), after optimization: {0.5 * np.sum(final_errors**2):.1f} pixels')\n",
    "assert(np.isclose(0.5 * np.sum(final_errors**2), result.error()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show error histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(initial_errors, np.linspace(0, 50, 20), alpha=0.5, label='before optimization')\n",
    "ax.hist(final_errors, np.linspace(0, 50, 20), alpha=0.5, label='after optimization')\n",
    "ax.legend()\n",
    "ax.set_xlabel('error (pixels)')\n",
    "ax.set_ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save annotated images to show match (hopefully) between given and projected image points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose parameters for annotation\n",
    "text_offset = 10\n",
    "mark_radius = 10\n",
    "text_scale = 1\n",
    "text_thickness = 3\n",
    "text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "text_linetype = cv2.LINE_AA\n",
    "\n",
    "# Iterate over all views\n",
    "for view_data, view_initial, view_final in zip(views, result.initial_values['views'], result.optimized_values['views']):\n",
    "    # Image paths\n",
    "    img_src_path = Path(img_src_dir, view_data['image_name'])\n",
    "    img_dst_path = Path(img_dst_dir, view_data['image_name'])\n",
    "    print(f'{img_src_path} -> {img_dst_path}')\n",
    "\n",
    "    # Read image as BGR\n",
    "    img = cv2.imread(str(img_src_path))\n",
    "    \n",
    "    # Add annotations to image\n",
    "    for i_match, match in enumerate(view_final['matches']):\n",
    "        # Get the image point that was given\n",
    "        q = match['q']\n",
    "        \n",
    "        # Get the image point that was computed by projection (before optimization)\n",
    "        q_initial = projection_num(\n",
    "            view_initial['T'],\n",
    "            match['p'],\n",
    "            result.initial_values['fx'],\n",
    "            result.initial_values['fy'],\n",
    "            result.initial_values['cx'],\n",
    "            result.initial_values['cy'],\n",
    "            result.initial_values['epsilon'],\n",
    "        )\n",
    "\n",
    "        # Get the image point that was computed by projection (after optimization)\n",
    "        q_final = projection_num(\n",
    "            view_final['T'],\n",
    "            match['p'],\n",
    "            result.optimized_values['fx'],\n",
    "            result.optimized_values['fy'],\n",
    "            result.optimized_values['cx'],\n",
    "            result.optimized_values['cy'],\n",
    "            result.optimized_values['epsilon'],\n",
    "        )\n",
    "\n",
    "        # Mark and number the image point that was given\n",
    "        cv2.circle(\n",
    "            img,\n",
    "            (int(q[0]), int(q[1])),\n",
    "            2 * mark_radius,\n",
    "            (0, 0, 255),\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            f'{i_match}',\n",
    "            (int(q[0]) + text_offset, int(q[1]) + 4 * text_offset),\n",
    "            text_font,\n",
    "            text_scale,\n",
    "            (0, 0, 255),\n",
    "            text_thickness,\n",
    "            text_linetype,\n",
    "        )\n",
    "        \n",
    "        # Mark and number the image point that was computed by projection (before optimization)\n",
    "        cv2.circle(\n",
    "            img,\n",
    "            (int(q_initial[0]), int(q_initial[1])),\n",
    "            int(1.5 * mark_radius),\n",
    "            (0, 255, 0),\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            f'{i_match}',\n",
    "            (int(q_initial[0]) - 4 * text_offset, int(q_initial[1]) - 2 * text_offset),\n",
    "            text_font,\n",
    "            text_scale,\n",
    "            (0, 255, 0),\n",
    "            text_thickness,\n",
    "            text_linetype,\n",
    "        )\n",
    "\n",
    "        # Mark and number the image point that was computed by projection (after optimization)\n",
    "        cv2.circle(\n",
    "            img,\n",
    "            (int(q_final[0]), int(q_final[1])),\n",
    "            mark_radius,\n",
    "            (255, 0, 0),\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            f'{i_match}',\n",
    "            (int(q_final[0]) + text_offset, int(q_final[1]) - 2 * text_offset),\n",
    "            text_font,\n",
    "            text_scale,\n",
    "            (255, 0, 0),\n",
    "            text_thickness,\n",
    "            text_linetype,\n",
    "        )\n",
    "\n",
    "    cv2.imwrite(str(img_dst_path), img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
