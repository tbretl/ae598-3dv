{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numerical methods\n",
    "import numpy as np\n",
    "\n",
    "# For image processing and visualization of results\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "# For timing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images from files (example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify filenames\n",
    "img1_filename = 'image01.PNG'\n",
    "img2_filename = 'image02.PNG'\n",
    "\n",
    "# Read images\n",
    "img1 = cv2.imread(img1_filename, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(img2_filename, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get width and height from first image\n",
    "frame_width = img1.shape[1]\n",
    "frame_height = img1.shape[0]\n",
    "\n",
    "# Verify width and height of second image are the same\n",
    "assert(img2.shape[1] == frame_width)\n",
    "assert(img2.shape[0] == frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images from video (example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify filename\n",
    "video_filename = 'video.MOV'\n",
    "\n",
    "# Create a video reader\n",
    "video_src = cv2.VideoCapture(video_filename)\n",
    "\n",
    "# Say what frames we want to read\n",
    "# - index of first frame\n",
    "i_frame_1 = 0\n",
    "# - index of last frame\n",
    "i_frame_2 = int(video_src.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "# Read first frame\n",
    "video_src.set(cv2.CAP_PROP_POS_FRAMES, i_frame_1)\n",
    "success, frame = video_src.read()\n",
    "assert(success)\n",
    "img1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Read second frame\n",
    "video_src.set(cv2.CAP_PROP_POS_FRAMES, i_frame_2)\n",
    "success, frame = video_src.read()\n",
    "assert(success)\n",
    "img2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do detection and matching with SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SIFT feature detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Apply detector to find keypoints (pts) and descriptors (desc) in each image\n",
    "start_time = time.time()\n",
    "pts1, desc1 = sift.detectAndCompute(image=img1, mask=None)\n",
    "pts2, desc2 = sift.detectAndCompute(image=img2, mask=None)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Elapsed time for detection (seconds): {elapsed_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keypoints are returned as a [tuple](https://docs.python.org/3/library/stdtypes.html#typesseq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first keypoint that was found in the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing for us is where this keypoint is located (in image coordinates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1[0].pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can count the number of elements in each tuple of keypoints just like you would in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Found {len(pts1)} features in img1')\n",
    "print(f'Found {len(pts2)} features in img2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also iterate through each tuple of keypoints just like you would iterate through a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pts1:\n",
    "    print(p.pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all detected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# Show each image in its respective axis\n",
    "ax1.imshow(img1, cmap='gray')\n",
    "ax2.imshow(img2, cmap='gray')\n",
    "\n",
    "# FIXME: Plot all features detected in img1 on ax1 (e.g., as red dots)\n",
    "for p in pts1:\n",
    "    pass\n",
    "\n",
    "# FIXME: Plot all features detected in img2 on ax2 (e.g., as red dots)\n",
    "for p in pts2:\n",
    "    pass\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME.** Answer the following questions:\n",
    "* Where are the features in each image?\n",
    "* Where *aren't* the features in each image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV has its own way of visualizing detected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# Show each image with detected features in its respective axis\n",
    "ax1.imshow(cv2.drawKeypoints(img1, pts1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
    "ax2.imshow(cv2.drawKeypoints(img1, pts1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each keypoint is associated with a descriptor. Descriptors are rows in a 2D numpy array. Let's look at the descriptor associated with the first keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is its length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(desc1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute force matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do \"brute force\" matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a brute-force matcher\n",
    "bf = cv2.BFMatcher(\n",
    "    normType=cv2.NORM_L2,\n",
    "    crossCheck=True,\n",
    ")\n",
    "\n",
    "# Use brute-force matcher to find matching descriptors\n",
    "start_time = time.time()\n",
    "matches = bf.match(desc1, desc2)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Elapsed time for matching (seconds): {elapsed_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matches are returned as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'found {len(matches)} matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first match that was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each match has three things that are important:\n",
    "* The index of a keypoint (and descriptor) in the first image\n",
    "* The index of a keypoint (and descriptor) in the second image\n",
    "* The distance between the descriptors of these two keypoints\n",
    "\n",
    "Here are those three things for the first match found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of keypoint/descriptor in first image\n",
    "idx1 = matches[0].queryIdx\n",
    "\n",
    "# Index of keypoint/descriptor in second image\n",
    "idx2 = matches[0].trainIdx\n",
    "\n",
    "# Distance between descriptors\n",
    "d = matches[0].distance\n",
    "\n",
    "print(f'KP {idx1} in img1 matched KP {idx2} in img2 (distance = {d:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we specified `normType=cv2.NORM_L2` when creating the matcher, the distance between two descriptors is simply the 2-norm (i.e., the standard Euclidean norm) of their difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME - compute the norm of the difference between the descriptors associated with the first match\n",
    "d_check = 0.\n",
    "\n",
    "# Check that it is the same as the distance associated with the first match\n",
    "print(f'distance:\\n {d:12.8f} (from match)\\n {d_check:12.8f} (from 2-norm of difference between descriptors)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two keypoints are a match if the distance between their descriptors is (1) smallest, and (2) below some threshold. We usually want to sort the matches by their distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort matches by distance (smallest first)\n",
    "matches = sorted(matches, key = lambda m: m.distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# Show each image in its respective axis\n",
    "ax1.imshow(img1, cmap='gray')\n",
    "ax2.imshow(img2, cmap='gray')\n",
    "\n",
    "# Visualize match\n",
    "# - Get first match\n",
    "m = matches[0]\n",
    "# - Get location of keypoints associated with first match\n",
    "p1 = pts1[m.queryIdx].pt\n",
    "p2 = pts2[m.trainIdx].pt\n",
    "# - Plot location of each keypoint as a red dot\n",
    "ax1.plot(p1[0], p1[1], 'r.', markersize=12)\n",
    "ax2.plot(p2[0], p2[1], 'r.', markersize=12)\n",
    "# - Zoom in on location of each keypoint\n",
    "s = 10 # <-- FIXME: change if necessary\n",
    "ax1.set_xlim(p1[0] - s, p1[0] + s)\n",
    "ax1.set_ylim(p1[1] + s, p1[1] - s)\n",
    "ax2.set_xlim(p2[0] - s, p2[0] + s)\n",
    "ax2.set_ylim(p2[1] + s, p2[1] - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the worst match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the $n$ best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of matches to show\n",
    "n = 50\n",
    "\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# Show images\n",
    "ax1.imshow(img1, cmap='gray')\n",
    "ax2.imshow(img2, cmap='gray')\n",
    "\n",
    "# Show first match (FIXME: modify this code to show the n best matches)\n",
    "# - Get first match\n",
    "m = matches[0]\n",
    "# - Get location of keypoints associated with first match\n",
    "p1 = pts1[m.queryIdx].pt\n",
    "p2 = pts2[m.trainIdx].pt\n",
    "# - Draw line connecting keypoint in first image with keypoint in second image\n",
    "fig.add_artist(\n",
    "    ConnectionPatch(\n",
    "        p1, p2, \n",
    "        'data', 'data',\n",
    "        axesA=ax1, axesB=ax2,\\\n",
    "        color='red',\n",
    "        connectionstyle='arc3, rad=0.',\n",
    "        linewidth=0.5,\n",
    "    )\n",
    ")\n",
    "# - Draw red dot at each keypoint\n",
    "ax1.plot(p1[0], p1[1], 'r.', markersize=2)\n",
    "ax2.plot(p2[0], p2[1], 'r.', markersize=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME.** Answer the following questions:\n",
    "* Was the \"best\" match actually good?\n",
    "* Was the \"worst\" match actually bad?\n",
    "* How many of the $n$ best matches are actually good?\n",
    "* What can you observe about the lines that correspond to the $n$ best matches? Describe them in words. What would you want this set of lines to look like if the $n$ best matches are actually good? Do they look like this in your case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matching with kNN ($k$ nearest neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of finding only the \"best\" match for each descriptor, we can find the $k=2$ best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a brute-force matcher\n",
    "bf = cv2.BFMatcher(\n",
    "    normType=cv2.NORM_L2,\n",
    "    crossCheck=False,       # <-- IMPORTANT - must be False for kNN matching\n",
    ")\n",
    "\n",
    "# Find the two best matches between descriptors (with distance below some threshold)\n",
    "start_time = time.time()\n",
    "matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Elapsed time for matching (seconds): {elapsed_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matches is now a tuple of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is actually two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at these two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = matches[0][0].queryIdx\n",
    "idx2 = matches[0][0].trainIdx\n",
    "d = matches[0][0].distance\n",
    "print(f'(idx1 = {idx1}, idx2 = {idx2}) : distance = {d}')\n",
    "\n",
    "idx1 = matches[0][1].queryIdx\n",
    "idx2 = matches[0][1].trainIdx\n",
    "d = matches[0][1].distance\n",
    "print(f'(idx1 = {idx1}, idx2 = {idx2}) : distance = {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `idx1` is the same in both cases. You can think of these as two different candidate matches for the keypoint with index `idx1`. We would prefer to be certain about matches. Notice that the first distance is less than the second distance. That is, the first candidate is the \"first best\" match in the second image for the keypoint with index `idx1` in the first image, and the second candidate is the \"second best\" match.\n",
    "\n",
    "**FIXME.** Answer the following question:\n",
    "* What relationship between the distances would indicate greater certainty that the \"first best\" candidate match is actually good?\n",
    "\n",
    "Implement your condition (hint - often called the \"ratio test\") to create a subset of \"good matches.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    # m is the \"first best\" match\n",
    "    # n is the \"second best\" match\n",
    "    if True: # <-- FIXME: replace with your condition on m.distance and n.distance\n",
    "        good_matches.append(m)\n",
    "\n",
    "print(f'found {len(good_matches)} good matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME.** Answer the following question:\n",
    "* How does the number of good matches vary with the ratio in your ratio test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort `good_matches` by distance and rename as `matches` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort matches by distance (smallest first)\n",
    "matches = sorted(good_matches, key = lambda m: m.distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the best good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the worst good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME.** Answer the following questions:\n",
    "* Was the \"best good match\" actually good?\n",
    "* Was the \"worst good match\" actually bad?\n",
    "* How many of the good matches are actually good?\n",
    "* What can you observe about the lines that correspond to the good matches? Do they look different than the lines you saw before? Do they look more (or less) like what you want?\n",
    "* Which of your answers would change if you changed the threshold ratio in your ratio test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the points in the source image (`img1`) and the target image (`img2`) that correspond to all the good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_src = []\n",
    "pts_dst = []\n",
    "for m in matches:\n",
    "    idx1 = m.queryIdx\n",
    "    idx2 = m.trainIdx\n",
    "    pts_src.append(pts1[idx1].pt)\n",
    "    pts_dst.append(pts2[idx2].pt)\n",
    "pts_src = np.array(pts_src)\n",
    "pts_dst = np.array(pts_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These points ($p$ from `pts_src` and $q$ from `pts_dst`), when expressed in homogeneous coordinates, are related by a homography:\n",
    "\n",
    "$$\\begin{bmatrix} q \\\\ 1 \\end{bmatrix} \\sim H \\begin{bmatrix} p \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "Use your code from HW1 to estimate this homography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "# Show target image\n",
    "ax.imshow(img2, cmap='gray')\n",
    "\n",
    "# Compare predicted and actual location of matched points in the target image\n",
    "for p, q in zip(pts_src, pts_dst):\n",
    "    # FIXME - Use homography to predict q from p\n",
    "    q_pred = q.copy()\n",
    "\n",
    "    # Plot the actual q and the predicted q\n",
    "    ax.plot(q[0], q[1], 'b.', markersize=18)\n",
    "    ax.plot(q_pred[0], q_pred[1], 'r.', markersize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do detection and matching with ORB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ORB feature detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Apply detector to find keypoints (pts) and descriptors (desc) in each image\n",
    "start_time = time.time()\n",
    "pts1, desc1 = orb.detectAndCompute(image=img1, mask=None)\n",
    "pts2, desc2 = orb.detectAndCompute(image=img2, mask=None)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Elapsed time for detection (seconds): {elapsed_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say how many keypoints were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all detected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute force matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the $k=2$ best matches for each keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a brute-force matcher\n",
    "bf = cv2.BFMatcher(\n",
    "    normType=cv2.NORM_HAMMING,   # <-- IMPORTANT - the ORB descriptor is binary, so we use hamming distance rather than L2 distance\n",
    "    crossCheck=False,            # <-- IMPORTANT - must be False for kNN matching\n",
    ")\n",
    "\n",
    "# Find the two best matches between descriptors (with distance below some threshold)\n",
    "start_time = time.time()\n",
    "matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Elapsed time for matching (seconds): {elapsed_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the subset of good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort `good_matches` by distance and rename as `matches` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the best good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the worst good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the points in the source image (`img1`) and the target image (`img2`) that correspond to all the good matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the homography between source and target images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME.** Compare results with ORB to results with SIFT, for example in terms of the following things:\n",
    "* Computation time?\n",
    "* Number of matches found?\n",
    "* Extent to which good matches were actually good?\n",
    "* Homography estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME.** Do the following:\n",
    "* Search for the 2022 edition (important!) of \"Computer vision algorithms and applications\" by Szeliski on the [university library website](https://library.illinois.edu)\n",
    "* Download the complete PDF of this book\n",
    "* Read Chapter 7.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
